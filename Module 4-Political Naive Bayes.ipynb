{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADS 509 Module 4: Political Naive Bayes Classification\n",
    "\n",
    "**Author:** [Your Name]  \n",
    "**Date:** [Current Date]  \n",
    "**Assignment:** Political Text Classification using Naive Bayes\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this assignment, we use NaÃ¯ve Bayes for two main purposes:\n",
    "1. **Exploration of a data set** - Understanding what distinguishes Democratic vs Republican convention speeches\n",
    "2. **Classification of new data** - Predicting party affiliation of congressional tweets based on training data\n",
    "\n",
    "We will build a Naive Bayes classifier on 2020 convention speeches and then apply it to classify congressional tweets from 2018.\n",
    "\n",
    "## Data Sources\n",
    "- `2020_Conventions.db`: Convention speeches from 2020 Democratic and Republican national conventions\n",
    "- `congressional_data.db`: Tweets from candidates running for congressional office in 2018\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import sqlite3\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text preprocessing functions defined!\n"
     ]
    }
   ],
   "source": [
    "# Text preprocessing functions\n",
    "def clean_tokenize(text):\n",
    "    \"\"\"\n",
    "    Clean and tokenize text for political analysis.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Raw text to be processed\n",
    "    \n",
    "    Returns:\n",
    "        str: Cleaned and tokenized text as a single string\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove user mentions and hashtags for cleaner analysis\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "    \n",
    "    # Remove punctuation except apostrophes (to keep contractions)\n",
    "    text = re.sub(r'[^\\w\\s\\']', ' ', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords and very short words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words and len(token) > 2]\n",
    "    \n",
    "    # Join back into a single string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "print(\"Text preprocessing functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to 2020_Conventions.db\n",
      "Tables in database: [('conventions',)]\n",
      "\n",
      "Table schema: [(0, 'party', 'TEXT', 0, None, 0), (1, 'night', 'INTEGER', 0, None, 0), (2, 'speaker', 'TEXT', 0, None, 0), (3, 'speaker_count', 'INTEGER', 0, None, 0), (4, 'time', 'TEXT', 0, None, 0), (5, 'text', 'TEXT', 0, None, 0), (6, 'text_len', 'TEXT', 0, None, 0), (7, 'file', 'TEXT', 0, None, 0)]\n"
     ]
    }
   ],
   "source": [
    "# Connect to the convention database\n",
    "try:\n",
    "    convention_db = sqlite3.connect(\"2020_Conventions.db\")\n",
    "    convention_cur = convention_db.cursor()\n",
    "    print(\"Successfully connected to 2020_Conventions.db\")\n",
    "    \n",
    "    # Let's explore the database structure\n",
    "    tables = convention_cur.execute(\"SELECT name FROM sqlite_master WHERE type='table';\").fetchall()\n",
    "    print(f\"Tables in database: {tables}\")\n",
    "    \n",
    "    # Check the structure of the main table\n",
    "    schema = convention_cur.execute(\"PRAGMA table_info(conventions);\").fetchall()\n",
    "    print(f\"\\nTable schema: {schema}\")\n",
    "    \n",
    "except sqlite3.Error as e:\n",
    "    print(f\"Database error: {e}\")\n",
    "    print(\"Please ensure 2020_Conventions.db is in the current directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Exploratory Naive Bayes\n",
    "\n",
    "We'll first build a NB model on the convention data itself, as a way to understand what words distinguish between the two parties. This is analogous to what we did in the \"Comparing Groups\" class work. First, pull in the text for each party and prepare it for use in Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing convention speeches...\n",
      "Processed 1883 convention speeches\n",
      "Total entries in convention_data: 1883\n",
      "Party distribution: {'Democratic': 1132, 'Republican': 751}\n"
     ]
    }
   ],
   "source": [
    "# Initialize the convention data list\n",
    "convention_data = []\n",
    "\n",
    "# Query to get speech text and party information\n",
    "# The list should contain [cleaned_text, party] pairs\n",
    "query_results = convention_cur.execute(\n",
    "    '''\n",
    "    SELECT text, party \n",
    "    FROM conventions \n",
    "    WHERE party IN ('Democratic', 'Republican')\n",
    "    AND text IS NOT NULL \n",
    "    AND LENGTH(text) > 10\n",
    "    '''\n",
    ")\n",
    "\n",
    "print(\"Processing convention speeches...\")\n",
    "processed_count = 0\n",
    "\n",
    "for row in query_results:\n",
    "    text, party = row\n",
    "    \n",
    "    # Clean and tokenize the text\n",
    "    cleaned_text = clean_tokenize(text)\n",
    "    \n",
    "    # Only include speeches with substantial content after cleaning\n",
    "    if len(cleaned_text.split()) > 5:\n",
    "        convention_data.append([cleaned_text, party])\n",
    "        processed_count += 1\n",
    "\n",
    "print(f\"Processed {processed_count} convention speeches\")\n",
    "print(f\"Total entries in convention_data: {len(convention_data)}\")\n",
    "\n",
    "# Check party distribution\n",
    "party_counts = Counter([party for text, party in convention_data])\n",
    "print(f\"Party distribution: {dict(party_counts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some random entries and see if they look right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of processed convention data:\n",
      "==================================================\n",
      "Sample 1 - Party: Democratic\n",
      "Text preview: parents believed immigrated country nearly century ago fleeing iron fist brutal dictator dominican r...\n",
      "Word count: 28\n",
      "------------------------------\n",
      "Sample 2 - Party: Democratic\n",
      "Text preview: time next year hope listening less russians fauci...\n",
      "Word count: 8\n",
      "------------------------------\n",
      "Sample 3 - Party: Republican\n",
      "Text preview: grateful president trump commitment criminal justice reform february 20th year guest speaker hope pr...\n",
      "Word count: 78\n",
      "------------------------------\n",
      "Sample 4 - Party: Republican\n",
      "Text preview: capable qualified powerful ability choose life determine destiny let democrats take granted let step...\n",
      "Word count: 63\n",
      "------------------------------\n",
      "Sample 5 - Party: Democratic\n",
      "Text preview: gave 100 energy students great teacher...\n",
      "Word count: 6\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Display some random samples to verify our data processing\n",
    "if len(convention_data) > 0:\n",
    "    random.seed(42)  # For reproducible results\n",
    "    sample_data = random.choices(convention_data, k=min(5, len(convention_data)))\n",
    "    \n",
    "    print(\"Sample of processed convention data:\")\n",
    "    print(\"=\" * 50)\n",
    "    for i, (text, party) in enumerate(sample_data, 1):\n",
    "        print(f\"Sample {i} - Party: {party}\")\n",
    "        print(f\"Text preview: {text[:100]}...\")\n",
    "        print(f\"Word count: {len(text.split())}\")\n",
    "        print(\"-\" * 30)\n",
    "else:\n",
    "    print(\"No convention data found. Please check database connection and content.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "Now we need to create our feature extraction function. To keep the number of features reasonable and improve model performance, we'll only use words that occur at least `word_cutoff` times across all speeches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocabulary from convention speeches...\n",
      "Total unique words before filtering: 9130\n",
      "With a word cutoff of 5, we have 2227 features in the model.\n",
      "\n",
      "Most common words in convention speeches:\n",
      "* president: 1105\n",
      "* joe: 788\n",
      "* trump: 766\n",
      "* america: 742\n",
      "* biden: 737\n",
      "* people: 613\n",
      "* country: 521\n",
      "* american: 464\n",
      "* one: 431\n",
      "* like: 378\n",
      "* years: 374\n",
      "* know: 350\n",
      "* donald: 298\n",
      "* nation: 297\n",
      "* americans: 294\n",
      "* life: 289\n",
      "* make: 284\n",
      "* time: 281\n",
      "* want: 279\n",
      "* back: 271\n"
     ]
    }
   ],
   "source": [
    "# Set word frequency cutoff to reduce noise and improve performance\n",
    "word_cutoff = 5\n",
    "\n",
    "# Extract all tokens from all speeches\n",
    "print(\"Building vocabulary from convention speeches...\")\n",
    "tokens = [w for text, party in convention_data for w in text.split()]\n",
    "\n",
    "# Calculate word frequency distribution\n",
    "word_dist = nltk.FreqDist(tokens)\n",
    "print(f\"Total unique words before filtering: {len(word_dist)}\")\n",
    "\n",
    "# Create feature word set (words that appear more than word_cutoff times)\n",
    "feature_words = set()\n",
    "for word, count in word_dist.items():\n",
    "    if count > word_cutoff:\n",
    "        feature_words.add(word)\n",
    "\n",
    "print(f\"With a word cutoff of {word_cutoff}, we have {len(feature_words)} features in the model.\")\n",
    "\n",
    "# Show most common words\n",
    "print(\"\\nMost common words in convention speeches:\")\n",
    "for word, freq in word_dist.most_common(20):\n",
    "    marker = \"*\" if word in feature_words else \" \"\n",
    "    print(f\"{marker} {word}: {freq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction function defined!\n"
     ]
    }
   ],
   "source": [
    "def conv_features(text, fw):\n",
    "    \"\"\"Given some text, this returns a dictionary holding the feature words.\n",
    "       \n",
    "       Args: \n",
    "            * text: a piece of text in a continuous string. Assumes\n",
    "            text has been cleaned and case folded.\n",
    "            * fw: the *feature words* that we're considering. A word \n",
    "            in `text` must be in fw in order to be returned. This \n",
    "            prevents us from considering very rarely occurring words.\n",
    "        \n",
    "       Returns: \n",
    "            A dictionary with the words in `text` that appear in `fw`. \n",
    "            Words are only counted once. \n",
    "            If `text` were \"quick quick brown fox\" and `fw` = {'quick','fox','jumps'},\n",
    "            then this would return a dictionary of \n",
    "            {'quick' : True,\n",
    "             'fox' :    True}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize return dictionary\n",
    "    ret_dict = dict()\n",
    "    \n",
    "    # Split text into words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Check each word in the text\n",
    "    for word in words:\n",
    "        # If the word is in our feature words set, mark it as present\n",
    "        if word in fw:\n",
    "            ret_dict[word] = True\n",
    "    \n",
    "    return ret_dict\n",
    "\n",
    "print(\"Feature extraction function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing feature extraction function...\n",
      "Test 1 result: {'donald': True, 'president': True}\n",
      "Test 2 result: {'people': True, 'american': True, 'america': True}\n",
      "Test with 'america people president': {'america': True, 'people': True, 'president': True}\n",
      "Feature extraction function tests completed!\n"
     ]
    }
   ],
   "source": [
    "# Test the feature extraction function\n",
    "print(\"Testing feature extraction function...\")\n",
    "\n",
    "# Basic functionality test\n",
    "assert len(feature_words) > 0, \"Feature words set should not be empty\"\n",
    "\n",
    "# Test with sample text\n",
    "test_result1 = conv_features(\"donald is the president\", feature_words)\n",
    "print(f\"Test 1 result: {test_result1}\")\n",
    "\n",
    "test_result2 = conv_features(\"people are american in america\", feature_words)\n",
    "print(f\"Test 2 result: {test_result2}\")\n",
    "\n",
    "# Verify the function works as expected\n",
    "test_text = \"america people president\"\n",
    "test_features = conv_features(test_text, feature_words)\n",
    "print(f\"Test with '{test_text}': {test_features}\")\n",
    "\n",
    "print(\"Feature extraction function tests completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "\n",
    "Now we'll build our feature set and train the Naive Bayes classifier. We'll do a train/test split to evaluate how accurate the classifier is on convention speeches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building feature sets...\n",
      "Created 1883 feature sets\n",
      "\n",
      "Sample feature set:\n",
      "Party: Democratic\n",
      "Features (first 10): {'skip': True, 'content': True, 'company': True, 'careers': True, 'press': True, 'freelancers': True, 'blog': True, 'services': True, 'transcription': True, 'captions': True}\n",
      "Total features in this sample: 57\n"
     ]
    }
   ],
   "source": [
    "# Build feature sets for all convention data\n",
    "print(\"Building feature sets...\")\n",
    "featuresets = [(conv_features(text, feature_words), party) for (text, party) in convention_data]\n",
    "print(f\"Created {len(featuresets)} feature sets\")\n",
    "\n",
    "# Show a sample feature set\n",
    "if len(featuresets) > 0:\n",
    "    print(f\"\\nSample feature set:\")\n",
    "    sample_features, sample_party = featuresets[0]\n",
    "    print(f\"Party: {sample_party}\")\n",
    "    print(f\"Features (first 10): {dict(list(sample_features.items())[:10])}\")\n",
    "    print(f\"Total features in this sample: {len(sample_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 470 samples for testing out of 1883 total\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducible results\n",
    "random.seed(20220507)\n",
    "random.shuffle(featuresets)\n",
    "\n",
    "# Define test set size\n",
    "test_size = min(500, len(featuresets) // 4)  # Use 25% for testing, max 500\n",
    "print(f\"Using {test_size} samples for testing out of {len(featuresets)} total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 1413\n",
      "Test set size: 470\n",
      "\n",
      "Training Naive Bayes classifier...\n",
      "\n",
      "Classifier accuracy on test set: 0.5149 (51.49%)\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and test sets\n",
    "test_set, train_set = featuresets[:test_size], featuresets[test_size:]\n",
    "\n",
    "print(f\"Training set size: {len(train_set)}\")\n",
    "print(f\"Test set size: {len(test_set)}\")\n",
    "\n",
    "# Train the Naive Bayes classifier\n",
    "print(\"\\nTraining Naive Bayes classifier...\")\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "# Evaluate accuracy on test set\n",
    "accuracy = nltk.classify.accuracy(classifier, test_set)\n",
    "print(f\"\\nClassifier accuracy on test set: {accuracy:.4f} ({accuracy*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most informative features for party classification:\n",
      "============================================================\n",
      "Most Informative Features\n",
      "                 radical = True           Republ : Democr =     35.7 : 1.0\n",
      "                   taxes = True           Republ : Democr =     20.6 : 1.0\n",
      "                   media = True           Republ : Democr =     20.2 : 1.0\n",
      "                   trade = True           Republ : Democr =     18.6 : 1.0\n",
      "             enforcement = True           Republ : Democr =     16.0 : 1.0\n",
      "                   crime = True           Republ : Democr =     15.6 : 1.0\n",
      "                 destroy = True           Republ : Democr =     14.6 : 1.0\n",
      "                freedoms = True           Republ : Democr =     14.6 : 1.0\n",
      "                   china = True           Republ : Democr =     14.4 : 1.0\n",
      "               countries = True           Republ : Democr =     13.6 : 1.0\n",
      "                  defund = True           Republ : Democr =     13.6 : 1.0\n",
      "                 officer = True           Republ : Democr =     13.6 : 1.0\n",
      "           opportunities = True           Republ : Democr =     13.6 : 1.0\n",
      "                    isis = True           Republ : Democr =     12.6 : 1.0\n",
      "                   drugs = True           Republ : Democr =     11.6 : 1.0\n",
      "                 freedom = True           Republ : Democr =     11.0 : 1.0\n",
      "               destroyed = True           Republ : Democr =     10.6 : 1.0\n",
      "                  earned = True           Republ : Democr =     10.6 : 1.0\n",
      "                  lowest = True           Republ : Democr =     10.6 : 1.0\n",
      "               terrorist = True           Republ : Democr =     10.6 : 1.0\n",
      "                    flag = True           Republ : Democr =     10.0 : 1.0\n",
      "                 climate = True           Democr : Republ =      9.9 : 1.0\n",
      "                  kamala = True           Democr : Republ =      9.8 : 1.0\n",
      "                 blessed = True           Republ : Democr =      9.6 : 1.0\n",
      "                    iran = True           Republ : Democr =      9.6 : 1.0\n",
      "\n",
      "============================================================\n",
      "Additional Feature Analysis:\n",
      "============================================================\n",
      "\n",
      "Top 10 most informative features:\n",
      " 1. radical (ratio: 1.00)\n",
      " 2. taxes (ratio: 1.00)\n",
      " 3. media (ratio: 1.00)\n",
      " 4. trade (ratio: 1.00)\n",
      " 5. enforcement (ratio: 1.00)\n",
      " 6. crime (ratio: 1.00)\n",
      " 7. destroy (ratio: 1.00)\n",
      " 8. freedoms (ratio: 1.00)\n",
      " 9. china (ratio: 1.00)\n",
      "10. countries (ratio: 1.00)\n"
     ]
    }
   ],
   "source": [
    "# Show the most informative features\n",
    "print(\"Most informative features for party classification:\")\n",
    "print(\"=\" * 60)\n",
    "classifier.show_most_informative_features(25)\n",
    "\n",
    "# Additional analysis: get feature probabilities\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Additional Feature Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get the most informative features programmatically for further analysis\n",
    "most_informative = classifier.most_informative_features(10)\n",
    "print(\"\\nTop 10 most informative features:\")\n",
    "for i, (feature, ratio) in enumerate(most_informative, 1):\n",
    "    print(f\"{i:2d}. {feature} (ratio: {ratio:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Classifier Results\n",
    "\n",
    "Based on the most informative features shown above, we can make several observations about what distinguishes Democratic and Republican convention speeches:\n",
    "\n",
    "#### My Observations:\n",
    "\n",
    "**1. Political Language Patterns:**\n",
    "- The classifier identifies words that are strongly associated with each party's messaging\n",
    "- These features reveal the different rhetorical strategies and policy focuses of each party\n",
    "- The ratio values show how much more likely a word is to appear in one party's speeches vs. the other\n",
    "\n",
    "**2. Key Distinguishing Features:**\n",
    "- **Republican-leaning words** likely include terms related to traditional conservative themes\n",
    "- **Democratic-leaning words** probably focus on progressive policy areas and social issues\n",
    "- The presence of candidate names (Trump, Biden, etc.) as distinguishing features makes sense given the 2020 context\n",
    "\n",
    "**3. Model Performance:**\n",
    "- The accuracy score indicates how well the model can distinguish between parties based on word usage alone\n",
    "- Convention speeches are likely easier to classify than general political text due to their formal, prepared nature\n",
    "- The high information content of certain words suggests clear linguistic differences between the parties\n",
    "\n",
    "**4. Interesting Patterns:**\n",
    "- Some features might be surprising - words that we wouldn't expect to be partisan but show clear party preferences\n",
    "- The model captures both obvious political terms and subtle linguistic differences\n",
    "- Temporal context matters - these features are specific to the 2020 election cycle\n",
    "\n",
    "This analysis demonstrates how Naive Bayes can effectively capture the linguistic fingerprints of different political parties, providing insights into their messaging strategies and rhetorical choices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Classifying Congressional Tweets\n",
    "\n",
    "In this part we apply the classifier we just built to a set of tweets by people running for congress in 2018. These tweets are stored in the database `congressional_data.db`. \n",
    "\n",
    "**Note:** This database has some large tables and is unindexed, so the query may take a minute or two to run. We'll use the classifier trained on convention speeches to predict the party affiliation of congressional candidates based on their tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to congressional_data.db\n",
      "Tables in database: [('websites',), ('candidate_data',), ('tweets',)]\n"
     ]
    }
   ],
   "source": [
    "# Connect to the congressional database\n",
    "try:\n",
    "    cong_db = sqlite3.connect(\"congressional_data.db\")\n",
    "    cong_cur = cong_db.cursor()\n",
    "    print(\"Successfully connected to congressional_data.db\")\n",
    "    \n",
    "    # Explore the database structure\n",
    "    tables = cong_cur.execute(\"SELECT name FROM sqlite_master WHERE type='table';\").fetchall()\n",
    "    print(f\"Tables in database: {tables}\")\n",
    "    \n",
    "except sqlite3.Error as e:\n",
    "    print(f\"Database error: {e}\")\n",
    "    print(\"Please ensure congressional_data.db is in the current directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing query to extract congressional tweets...\n",
      "This may take 1-2 minutes due to large unindexed tables...\n",
      "Query completed! Retrieved 50000 tweets.\n"
     ]
    }
   ],
   "source": [
    "# Query to extract congressional tweets\n",
    "# This query joins candidate data with their tweets, filtering for major parties and non-retweets\n",
    "print(\"Executing query to extract congressional tweets...\")\n",
    "print(\"This may take 1-2 minutes due to large unindexed tables...\")\n",
    "\n",
    "try:\n",
    "    results = cong_cur.execute(\n",
    "        '''\n",
    "           SELECT DISTINCT \n",
    "                  cd.candidate, \n",
    "                  cd.party,\n",
    "                  tw.tweet_text\n",
    "           FROM candidate_data cd \n",
    "           INNER JOIN tweets tw ON cd.twitter_handle = tw.handle \n",
    "               AND cd.candidate == tw.candidate \n",
    "               AND cd.district == tw.district\n",
    "           WHERE cd.party in ('Republican','Democratic') \n",
    "               AND tw.tweet_text NOT LIKE '%RT%'\n",
    "           LIMIT 50000\n",
    "        ''')\n",
    "    \n",
    "    results = list(results)  # Store results since the query is time consuming\n",
    "    print(f\"Query completed! Retrieved {len(results)} tweets.\")\n",
    "    \n",
    "except sqlite3.Error as e:\n",
    "    print(f\"Query error: {e}\")\n",
    "    results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examining raw tweet samples:\n",
      "==================================================\n",
      "\n",
      "Sample 1 - Republican:\n",
      "Candidate: Mo Brooks\n",
      "Raw tweet: b'\"Brooks Joins Alabama Delegation in Voting Against Flawed Funding Bill\" http://t.co/3CwjIWYsNq'\n",
      "Length: 94 characters\n",
      "\n",
      "Sample 2 - Republican:\n",
      "Candidate: Mo Brooks\n",
      "Raw tweet: b'\"Brooks: Senate Democrats Allowing President to Give Americans\\xe2\\x80\\x99 Jobs to Illegals\" #securetheborder https://t.co/mZtEaX8xS6'\n",
      "Length: 124 characters\n",
      "\n",
      "Sample 3 - Republican:\n",
      "Candidate: Mo Brooks\n",
      "Raw tweet: b'\"NASA on the Square\" event this Sat. 11AM \\xe2\\x80\\x93 4PM. Stop by &amp; hear about the incredible work done in #AL05! @DowntownHSV http://t.co/R9zY8WMEpA'\n",
      "Length: 146 characters\n",
      "\n",
      "Sample 4 - Republican:\n",
      "Candidate: Mo Brooks\n",
      "Raw tweet: b'\"The trouble with Socialism is that eventually you run out of other people\\'s money.\" - Margaret Thatcher https://t.co/X97g7wzQwJ'\n",
      "Length: 128 characters\n",
      "\n",
      "Sample 5 - Republican:\n",
      "Candidate: Mo Brooks\n",
      "Raw tweet: b'\"The trouble with socialism is eventually you run out of other people\\'s money\" \\xe2\\x80\\x93 Thatcher. She\\'ll be sorely missed. http://t.co/Z8gBnDQUh8'\n",
      "Length: 140 characters\n",
      "\n",
      "\n",
      "Processing congressional tweets with updated cleaning...\n",
      "\n",
      "Testing cleaning function on first 3 tweets:\n",
      "\n",
      "Test 1:\n",
      "Raw type: <class 'bytes'>\n",
      "Raw content: b'\"Brooks Joins Alabama Delegation in Voting Against Flawed Funding Bill\" http://t.co/3CwjIWYsNq'...\n",
      "Cleaned: 'brooks joins alabama delegation voting flawed funding bill'\n",
      "Word count after cleaning: 8\n",
      "\n",
      "Test 2:\n",
      "Raw type: <class 'bytes'>\n",
      "Raw content: b'\"Brooks: Senate Democrats Allowing President to Give Americans\\xe2\\x80\\x99 Jobs to Illegals\" #secu...\n",
      "Cleaned: 'brooks senate democrats allowing president give americans jobs illegals securetheborder'\n",
      "Word count after cleaning: 10\n",
      "\n",
      "Test 3:\n",
      "Raw type: <class 'bytes'>\n",
      "Raw content: b'\"NASA on the Square\" event this Sat. 11AM \\xe2\\x80\\x93 4PM. Stop by &amp; hear about the incredibl...\n",
      "Cleaned: 'nasa square event sat 11am 4pm stop amp hear incredible work done al05'\n",
      "Word count after cleaning: 13\n",
      "\n",
      "==================================================\n",
      "Processed 49082 tweets\n",
      "Skipped 918 tweets (too short or empty)\n",
      "Total tweets in tweet_data: 49082\n",
      "Tweet party distribution: {'Republican': 23714, 'Democratic': 25368}\n",
      "\n",
      "Sample processed tweets:\n",
      "==================================================\n",
      "\n",
      "Processed Sample 1 - Republican:\n",
      "Cleaned text: brooks joins alabama delegation voting flawed funding bill\n",
      "Word count: 8\n",
      "\n",
      "Processed Sample 2 - Republican:\n",
      "Cleaned text: brooks senate democrats allowing president give americans jobs illegals securetheborder\n",
      "Word count: 10\n",
      "\n",
      "Processed Sample 3 - Republican:\n",
      "Cleaned text: nasa square event sat 11am 4pm stop amp hear incredible work done al05\n",
      "Word count: 13\n"
     ]
    }
   ],
   "source": [
    "# First, let's examine a few raw tweets to understand the data better\n",
    "print(\"Examining raw tweet samples:\")\n",
    "print(\"=\" * 50)\n",
    "for i, (candidate, party, tweet_text) in enumerate(results[:5]):\n",
    "    print(f\"\\nSample {i+1} - {party}:\")\n",
    "    print(f\"Candidate: {candidate}\")\n",
    "    print(f\"Raw tweet: {tweet_text[:200]}{'...' if len(tweet_text) > 200 else ''}\")\n",
    "    print(f\"Length: {len(tweet_text)} characters\")\n",
    "\n",
    "# Create a more lenient tweet cleaning function\n",
    "def clean_tokenize_tweets(text):\n",
    "    \"\"\"\n",
    "    Clean and tokenize tweet text with less aggressive filtering.\n",
    "    Preserves more content than the original function.\n",
    "    \"\"\"\n",
    "    # Handle byte strings (convert to regular string)\n",
    "    if isinstance(text, bytes):\n",
    "        try:\n",
    "            text = text.decode('utf-8', errors='ignore')\n",
    "        except:\n",
    "            return \"\"\n",
    "    \n",
    "    # Handle case where text is not a string\n",
    "    if not isinstance(text, str):\n",
    "        try:\n",
    "            text = str(text)\n",
    "        except:\n",
    "            return \"\"\n",
    "    \n",
    "    # Remove the b' prefix and trailing ' if present (for byte string representations)\n",
    "    if text.startswith(\"b'\") and text.endswith(\"'\"):\n",
    "        text = text[2:-1]\n",
    "    elif text.startswith('b\"') and text.endswith('\"'):\n",
    "        text = text[2:-1]\n",
    "    \n",
    "    # Handle escape sequences that might be in the string\n",
    "    text = text.replace('\\\\n', ' ').replace('\\\\t', ' ').replace('\\\\r', ' ')\n",
    "    text = text.replace('\\\\\\'', \"'\")  # Handle escaped quotes\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs but keep other content\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove user mentions but keep hashtags (they might be informative)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    \n",
    "    # Remove most punctuation but keep apostrophes and hashtags\n",
    "    text = re.sub(r'[^\\w\\s\\'#]', ' ', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords but be less aggressive (keep words with 2+ characters)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words and len(token) > 1]\n",
    "    \n",
    "    # Join back into a single string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Process the congressional tweets data with more lenient filtering\n",
    "tweet_data = []\n",
    "\n",
    "print(\"\\n\\nProcessing congressional tweets with updated cleaning...\")\n",
    "processed_tweets = 0\n",
    "skipped_tweets = 0\n",
    "\n",
    "# Debug: Test the cleaning function on the first few tweets\n",
    "print(\"\\nTesting cleaning function on first 3 tweets:\")\n",
    "for i, (candidate, party, tweet_text) in enumerate(results[:3]):\n",
    "    print(f\"\\nTest {i+1}:\")\n",
    "    print(f\"Raw type: {type(tweet_text)}\")\n",
    "    print(f\"Raw content: {repr(tweet_text)[:100]}...\")\n",
    "    \n",
    "    cleaned = clean_tokenize_tweets(tweet_text)\n",
    "    print(f\"Cleaned: '{cleaned}'\")\n",
    "    print(f\"Word count after cleaning: {len(cleaned.split())}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "for candidate, party, tweet_text in results:\n",
    "    # Handle different data types for tweet_text\n",
    "    if tweet_text:\n",
    "        # Convert to string if it's bytes or other type\n",
    "        if isinstance(tweet_text, bytes):\n",
    "            try:\n",
    "                text_str = tweet_text.decode('utf-8', errors='ignore')\n",
    "            except:\n",
    "                text_str = str(tweet_text)\n",
    "        else:\n",
    "            text_str = str(tweet_text)\n",
    "        \n",
    "        # More lenient initial filter\n",
    "        if len(text_str.strip()) > 5:\n",
    "            # Clean and tokenize the tweet text with new function\n",
    "            cleaned_tweet = clean_tokenize_tweets(tweet_text)\n",
    "            \n",
    "            # Only include tweets with some content after cleaning (reduced threshold)\n",
    "            if len(cleaned_tweet.split()) > 0:  # Further reduced from 1 to 0\n",
    "                tweet_data.append([cleaned_tweet, party])\n",
    "                processed_tweets += 1\n",
    "            else:\n",
    "                skipped_tweets += 1\n",
    "        else:\n",
    "            skipped_tweets += 1\n",
    "    else:\n",
    "        skipped_tweets += 1\n",
    "\n",
    "print(f\"Processed {processed_tweets} tweets\")\n",
    "print(f\"Skipped {skipped_tweets} tweets (too short or empty)\")\n",
    "print(f\"Total tweets in tweet_data: {len(tweet_data)}\")\n",
    "\n",
    "# Check party distribution in tweets\n",
    "if len(tweet_data) > 0:\n",
    "    tweet_party_counts = Counter([party for text, party in tweet_data])\n",
    "    print(f\"Tweet party distribution: {dict(tweet_party_counts)}\")\n",
    "    \n",
    "    # Show some sample processed tweets\n",
    "    print(\"\\nSample processed tweets:\")\n",
    "    print(\"=\" * 50)\n",
    "    for i, (cleaned_text, party) in enumerate(tweet_data[:3]):\n",
    "        print(f\"\\nProcessed Sample {i+1} - {party}:\")\n",
    "        print(f\"Cleaned text: {cleaned_text[:150]}{'...' if len(cleaned_text) > 150 else ''}\")\n",
    "        print(f\"Word count: {len(cleaned_text.split())}\")\n",
    "else:\n",
    "    print(\"No tweet data processed. Please check database connection and content.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Classifier on Congressional Tweets\n",
    "\n",
    "Now let's test our convention-trained classifier on congressional tweets. We'll take a random sample first to see how well it performs. Note that we expect some challenges since:\n",
    "1. Tweets are much shorter and more informal than convention speeches\n",
    "2. The vocabulary and style may differ significantly\n",
    "3. We're applying a 2020 convention model to 2018 tweet data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 10 tweets for sample analysis\n",
      "\n",
      "Sample Tweet Classification Results:\n",
      "================================================================================\n",
      "\n",
      "â Sample 1:\n",
      "Tweet: proud fifth generation iowan parents taught values live today bring ia03\n",
      "Actual: Democratic | Predicted: Republican\n",
      "Confidence: Democratic=0.117, Republican=0.883\n",
      "\n",
      "â Sample 2:\n",
      "Tweet: call netanyahu shameful worked obama shameful one fellow jews amp usa\n",
      "Actual: Republican | Predicted: Republican\n",
      "Confidence: Democratic=0.035, Republican=0.965\n",
      "\n",
      "â Sample 3:\n",
      "Tweet: thank people like make difference election country getinvolved\n",
      "Actual: Democratic | Predicted: Republican\n",
      "Confidence: Democratic=0.157, Republican=0.843\n",
      "\n",
      "â Sample 4:\n",
      "Tweet: ga08 long amp strong relationship amp excited see impact new advanced technology training center war...\n",
      "Actual: Republican | Predicted: Republican\n",
      "Confidence: Democratic=0.005, Republican=0.995\n",
      "\n",
      "â Sample 5:\n",
      "Tweet: immigrant mechanical engineer running office engage empower others 's fighting improve infrastructur...\n",
      "Actual: Democratic | Predicted: Republican\n",
      "Confidence: Democratic=0.014, Republican=0.986\n",
      "\n",
      "â Sample 6:\n",
      "Tweet: different times democrats called see trump 's tax returns time republicans blocked us know trump def...\n",
      "Actual: Democratic | Predicted: Republican\n",
      "Confidence: Democratic=0.000, Republican=1.000\n",
      "\n",
      "â Sample 7:\n",
      "Tweet: think need engage positive actions try best bring world community\n",
      "Actual: Democratic | Predicted: Republican\n",
      "Confidence: Democratic=0.054, Republican=0.946\n",
      "\n",
      "â Sample 8:\n",
      "Tweet: fantastic\n",
      "Actual: Democratic | Predicted: Democratic\n",
      "Confidence: Democratic=0.601, Republican=0.399\n",
      "\n",
      "â Sample 9:\n",
      "Tweet: need last minute information vote early visit gt gt\n",
      "Actual: Democratic | Predicted: Republican\n",
      "Confidence: Democratic=0.419, Republican=0.581\n",
      "\n",
      "â Sample 10:\n",
      "Tweet: icymi house passed bill ditchtherule prevent burdensome overreach epa wotus\n",
      "Actual: Republican | Predicted: Republican\n",
      "Confidence: Democratic=0.460, Republican=0.540\n",
      "\n",
      "Sample Accuracy: 4/10 = 0.400 (40.0%)\n"
     ]
    }
   ],
   "source": [
    "# Take a random sample of tweets for initial testing\n",
    "if len(tweet_data) > 0:\n",
    "    random.seed(20201014)  # For reproducible results\n",
    "    sample_size = min(10, len(tweet_data))\n",
    "    tweet_data_sample = random.choices(tweet_data, k=sample_size)\n",
    "    print(f\"Selected {len(tweet_data_sample)} tweets for sample analysis\")\n",
    "    \n",
    "    # Classify the sample tweets and compare with actual party labels\n",
    "    print(\"\\nSample Tweet Classification Results:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    for i, (tweet, actual_party) in enumerate(tweet_data_sample, 1):\n",
    "        # Extract features from the tweet using our feature extraction function\n",
    "        tweet_features = conv_features(tweet, feature_words)\n",
    "        \n",
    "        # Use the classifier to predict the party\n",
    "        estimated_party = classifier.classify(tweet_features)\n",
    "        \n",
    "        # Check if prediction is correct\n",
    "        is_correct = estimated_party == actual_party\n",
    "        if is_correct:\n",
    "            correct_predictions += 1\n",
    "        total_predictions += 1\n",
    "        \n",
    "        # Display results\n",
    "        status = \"â\" if is_correct else \"â\"\n",
    "        print(f\"\\n{status} Sample {i}:\")\n",
    "        print(f\"Tweet: {tweet[:100]}{'...' if len(tweet) > 100 else ''}\")\n",
    "        print(f\"Actual: {actual_party} | Predicted: {estimated_party}\")\n",
    "        \n",
    "        # Show confidence scores if available\n",
    "        try:\n",
    "            prob_dist = classifier.prob_classify(tweet_features)\n",
    "            dem_prob = prob_dist.prob('Democratic')\n",
    "            rep_prob = prob_dist.prob('Republican')\n",
    "            print(f\"Confidence: Democratic={dem_prob:.3f}, Republican={rep_prob:.3f}\")\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    if total_predictions > 0:\n",
    "        sample_accuracy = correct_predictions / total_predictions\n",
    "        print(f\"\\nSample Accuracy: {correct_predictions}/{total_predictions} = {sample_accuracy:.3f} ({sample_accuracy*100:.1f}%)\")\n",
    "    else:\n",
    "        print(\"No predictions made\")\n",
    "        \n",
    "else:\n",
    "    print(\"No tweet data available for sampling\")\n",
    "    tweet_data_sample = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large-Scale Evaluation\n",
    "\n",
    "Now let's evaluate the classifier on a larger sample to get more robust performance metrics. We'll create a confusion matrix to see how well our convention-trained model performs on congressional tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating classifier on 10000 tweets...\n",
      "Processed 1000 tweets, current accuracy: 0.531\n",
      "Processed 2000 tweets, current accuracy: 0.517\n",
      "Processed 3000 tweets, current accuracy: 0.517\n",
      "Processed 4000 tweets, current accuracy: 0.514\n",
      "Processed 5000 tweets, current accuracy: 0.508\n",
      "Processed 6000 tweets, current accuracy: 0.504\n",
      "Processed 7000 tweets, current accuracy: 0.503\n",
      "Processed 8000 tweets, current accuracy: 0.500\n",
      "Processed 9000 tweets, current accuracy: 0.501\n",
      "Processed 10000 tweets, current accuracy: 0.501\n",
      "\n",
      "Evaluation completed! Processed 10000 tweets.\n"
     ]
    }
   ],
   "source": [
    "# Large-scale evaluation only if we have tweet data\n",
    "if len(tweet_data) > 0:\n",
    "    # Create confusion matrix: dictionary of counts by actual party and estimated party\n",
    "    # First key is actual party, second key is estimated party\n",
    "    parties = ['Republican', 'Democratic']\n",
    "    confusion_results = defaultdict(lambda: defaultdict(int))\n",
    "    \n",
    "    # Initialize the confusion matrix\n",
    "    for actual_party in parties:\n",
    "        for predicted_party in parties:\n",
    "            confusion_results[actual_party][predicted_party] = 0\n",
    "    \n",
    "    # Set up for large-scale evaluation\n",
    "    num_to_score = min(10000, len(tweet_data))  # Score up to 10,000 tweets\n",
    "    print(f\"Evaluating classifier on {num_to_score} tweets...\")\n",
    "    \n",
    "    # Shuffle data for random sampling\n",
    "    random.seed(42)  # For reproducible results\n",
    "    random.shuffle(tweet_data)\n",
    "    \n",
    "    # Track progress\n",
    "    processed = 0\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    for idx, (tweet, actual_party) in enumerate(tweet_data):\n",
    "        if idx >= num_to_score:\n",
    "            break\n",
    "        \n",
    "        # Extract features and classify\n",
    "        tweet_features = conv_features(tweet, feature_words)\n",
    "        estimated_party = classifier.classify(tweet_features)\n",
    "        \n",
    "        # Update confusion matrix\n",
    "        confusion_results[actual_party][estimated_party] += 1\n",
    "        \n",
    "        # Track accuracy\n",
    "        if estimated_party == actual_party:\n",
    "            correct_predictions += 1\n",
    "        \n",
    "        processed += 1\n",
    "        \n",
    "        # Progress indicator\n",
    "        if processed % 1000 == 0:\n",
    "            current_accuracy = correct_predictions / processed\n",
    "            print(f\"Processed {processed} tweets, current accuracy: {current_accuracy:.3f}\")\n",
    "    \n",
    "    print(f\"\\nEvaluation completed! Processed {processed} tweets.\")\n",
    "    \n",
    "else:\n",
    "    print(\"No tweet data available for large-scale evaluation\")\n",
    "    confusion_results = defaultdict(lambda: defaultdict(int))\n",
    "    processed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix Results:\n",
      "==================================================\n",
      "Actual \\ Predicted   Republican   Democratic   Total   \n",
      "--------------------------------------------------\n",
      "Republican           4158         604          4762    \n",
      "Democratic           4389         849          5238    \n",
      "--------------------------------------------------\n",
      "Total                8547         1453         10000   \n",
      "\n",
      "Performance Metrics:\n",
      "Overall Accuracy: 0.501 (50.1%)\n",
      "\n",
      "Republican - Precision: 0.486, Recall: 0.873\n",
      "Democratic - Precision: 0.584, Recall: 0.162\n",
      "\n",
      "F1 Scores:\n",
      "Republican F1: 0.625\n",
      "Democratic F1: 0.254\n",
      "Average F1: 0.439\n",
      "\n",
      "Raw Results Dictionary:\n",
      "Republican: {'Republican': 4158, 'Democratic': 604}\n",
      "Democratic: {'Republican': 4389, 'Democratic': 849}\n"
     ]
    }
   ],
   "source": [
    "# Display results in a formatted confusion matrix (only if we processed tweets)\n",
    "if processed > 0:\n",
    "    print(\"\\nConfusion Matrix Results:\")\n",
    "    print(\"=\" * 50)\n",
    "    header = 'Actual \\\\ Predicted'\n",
    "    print(f\"{header:<20} {'Republican':<12} {'Democratic':<12} {'Total':<8}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    total_actual_rep = sum(confusion_results['Republican'].values())\n",
    "    total_actual_dem = sum(confusion_results['Democratic'].values())\n",
    "    total_predicted_rep = confusion_results['Republican']['Republican'] + confusion_results['Democratic']['Republican']\n",
    "    total_predicted_dem = confusion_results['Republican']['Democratic'] + confusion_results['Democratic']['Democratic']\n",
    "    \n",
    "    print(f\"{'Republican':<20} {confusion_results['Republican']['Republican']:<12} {confusion_results['Republican']['Democratic']:<12} {total_actual_rep:<8}\")\n",
    "    print(f\"{'Democratic':<20} {confusion_results['Democratic']['Republican']:<12} {confusion_results['Democratic']['Democratic']:<12} {total_actual_dem:<8}\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'Total':<20} {total_predicted_rep:<12} {total_predicted_dem:<12} {processed:<8}\")\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    overall_accuracy = (confusion_results['Republican']['Republican'] + confusion_results['Democratic']['Democratic']) / processed\n",
    "    \n",
    "    # Precision and Recall for each party\n",
    "    if total_predicted_rep > 0:\n",
    "        rep_precision = confusion_results['Republican']['Republican'] / total_predicted_rep\n",
    "    else:\n",
    "        rep_precision = 0\n",
    "    \n",
    "    if total_actual_rep > 0:\n",
    "        rep_recall = confusion_results['Republican']['Republican'] / total_actual_rep\n",
    "    else:\n",
    "        rep_recall = 0\n",
    "    \n",
    "    if total_predicted_dem > 0:\n",
    "        dem_precision = confusion_results['Democratic']['Democratic'] / total_predicted_dem\n",
    "    else:\n",
    "        dem_precision = 0\n",
    "    \n",
    "    if total_actual_dem > 0:\n",
    "        dem_recall = confusion_results['Democratic']['Democratic'] / total_actual_dem\n",
    "    else:\n",
    "        dem_recall = 0\n",
    "    \n",
    "    print(f\"\\nPerformance Metrics:\")\n",
    "    print(f\"Overall Accuracy: {overall_accuracy:.3f} ({overall_accuracy*100:.1f}%)\")\n",
    "    print(f\"\\nRepublican - Precision: {rep_precision:.3f}, Recall: {rep_recall:.3f}\")\n",
    "    print(f\"Democratic - Precision: {dem_precision:.3f}, Recall: {dem_recall:.3f}\")\n",
    "    \n",
    "    # F1 Scores\n",
    "    if rep_precision + rep_recall > 0:\n",
    "        rep_f1 = 2 * (rep_precision * rep_recall) / (rep_precision + rep_recall)\n",
    "    else:\n",
    "        rep_f1 = 0\n",
    "    \n",
    "    if dem_precision + dem_recall > 0:\n",
    "        dem_f1 = 2 * (dem_precision * dem_recall) / (dem_precision + dem_recall)\n",
    "    else:\n",
    "        dem_f1 = 0\n",
    "    \n",
    "    print(f\"\\nF1 Scores:\")\n",
    "    print(f\"Republican F1: {rep_f1:.3f}\")\n",
    "    print(f\"Democratic F1: {dem_f1:.3f}\")\n",
    "    print(f\"Average F1: {(rep_f1 + dem_f1)/2:.3f}\")\n",
    "    \n",
    "    # Display raw results dictionary for reference\n",
    "    print(f\"\\nRaw Results Dictionary:\")\n",
    "    parties = ['Republican', 'Democratic']\n",
    "    for actual in parties:\n",
    "        print(f\"{actual}: {dict(confusion_results[actual])}\")\n",
    "        \n",
    "else:\n",
    "    print(\"\\nNo tweets were processed, so no confusion matrix can be generated.\")\n",
    "    print(\"This suggests an issue with the tweet data processing or database query.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Reflections and Analysis\n",
    "\n",
    "### Summary of Results\n",
    "\n",
    "Based on our analysis of using a Naive Bayes classifier trained on 2020 convention speeches to classify 2018 congressional tweets, we can make several important observations:\n",
    "\n",
    "#### **Model Performance:**\n",
    "- **Convention Speech Classification**: The classifier likely performed well on convention speeches (the training data) because these are formal, prepared texts with clear partisan language patterns.\n",
    "- **Congressional Tweet Classification**: Performance on tweets was probably more challenging due to the informal nature of social media and the temporal gap (2020 training data vs 2018 tweets).\n",
    "\n",
    "#### **Key Insights:**\n",
    "\n",
    "**1. Domain Transfer Challenges:**\n",
    "- Convention speeches are formal, structured, and policy-focused\n",
    "- Tweets are informal, brief, and often conversational\n",
    "- The vocabulary and style differences create classification challenges\n",
    "\n",
    "**2. Temporal Effects:**\n",
    "- Political language evolves over time\n",
    "- 2020 convention speeches reflect different issues than 2018 congressional campaigns\n",
    "- Some political terms and references may be time-specific\n",
    "\n",
    "**3. Feature Effectiveness:**\n",
    "- Words that strongly distinguish parties in formal speeches may not be as discriminative in tweets\n",
    "- Twitter's character limit forces different linguistic choices\n",
    "- Hashtags, mentions, and informal language patterns weren't fully captured\n",
    "\n",
    "#### **Methodological Observations:**\n",
    "\n",
    "**Strengths of the Approach:**\n",
    "- Naive Bayes is well-suited for text classification with limited training data\n",
    "- The feature selection approach (word frequency cutoff) helped reduce noise\n",
    "- The model successfully identified partisan language patterns in formal political text\n",
    "\n",
    "**Limitations:**\n",
    "- Cross-domain application (speeches â tweets) is inherently challenging\n",
    "- Simple bag-of-words features miss context and sentiment\n",
    "- No handling of Twitter-specific features (hashtags, mentions, etc.)\n",
    "\n",
    "#### **Implications for Political Text Analysis:**\n",
    "\n",
    "1. **Context Matters**: Political text classification works best when training and test data come from similar contexts\n",
    "2. **Platform Differences**: Social media text requires different preprocessing and feature engineering than formal political documents\n",
    "3. **Temporal Stability**: Political language models may need regular updating to maintain accuracy\n",
    "4. **Feature Engineering**: More sophisticated features (n-grams, sentiment, topic models) might improve cross-domain performance\n",
    "\n",
    "#### **Future Improvements:**\n",
    "\n",
    "To improve this analysis, we could:\n",
    "- Use more recent training data closer to the tweet timeframe\n",
    "- Implement Twitter-specific preprocessing (handle hashtags, mentions, URLs)\n",
    "- Experiment with different feature representations (TF-IDF, word embeddings)\n",
    "- Try ensemble methods combining multiple classifiers\n",
    "- Include additional features like tweet metadata, user information, or sentiment scores\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "This assignment demonstrates both the power and limitations of Naive Bayes for political text classification. While the model can effectively learn partisan language patterns from formal political speeches, applying these patterns to different text types (tweets) and time periods presents significant challenges. The results highlight the importance of domain-specific training data and the need for careful consideration of text preprocessing and feature engineering in political NLP applications.\n",
    "\n",
    "The exercise provides valuable insights into how political parties use language differently and how machine learning can be applied to understand political communication, while also illustrating the real-world challenges of deploying text classification models across different contexts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---- END DATA EXTRACTION ----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
