{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5.1: Topic Modeling\n",
    "\n",
    "**Course:** ADS 509 - Applied Text Mining  \n",
    "**Assignment:** Topic Modeling with NMF, LSA, and LDA  \n",
    "**Student:** Gabriel Elohi  \n",
    "**Date:** $(date)\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this assignment, we will build and compare three different topic modeling approaches:\n",
    "1. **NMF (Non-negative Matrix Factorization)** model\n",
    "2. **LSA (Latent Semantic Analysis)** model  \n",
    "3. **LDA (Latent Dirichlet Allocation)** model\n",
    "\n",
    "We will work with the Brown University corpus from NLTK and compare the resulting topic allocations with the official document classifications.\n",
    "\n",
    "## AI Tool Attribution\n",
    "\n",
    "*If any AI tools (ChatGPT, Gemini, GitHub Copilot, etc.) are used in this assignment, they will be explicitly disclosed and cited here with explanations of their contributions.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Exploration\n",
    "\n",
    "Let's start by importing the necessary libraries and exploring the Brown corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NLTK imports\n",
    "import nltk\n",
    "from nltk.corpus import brown, stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "# Scikit-learn imports for topic modeling\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, TruncatedSVD, LatentDirichletAllocation\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('brown', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Exploring the Brown Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the Brown corpus structure\n",
    "print(\"Brown Corpus Categories:\")\n",
    "categories = brown.categories()\n",
    "print(f\"Number of categories: {len(categories)}\")\n",
    "print(f\"Categories: {categories}\")\n",
    "\n",
    "print(\"\\nDocument counts per category:\")\n",
    "for category in categories:\n",
    "    file_count = len(brown.fileids(categories=category))\n",
    "    print(f\"{category}: {file_count} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sample documents from different categories\n",
    "print(\"Sample documents from different categories:\")\n",
    "for i, category in enumerate(categories[:3]):\n",
    "    file_id = brown.fileids(categories=category)[0]\n",
    "    sample_text = ' '.join(brown.words(file_id)[:50])\n",
    "    print(f\"\\n{category.upper()} - {file_id}:\")\n",
    "    print(sample_text + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessing tools\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess text by tokenizing, removing stopwords, and lemmatizing.\n",
    "    \"\"\"\n",
    "    # Convert to lowercase and tokenize\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove non-alphabetic tokens and stopwords\n",
    "    tokens = [token for token in tokens if token.isalpha() and token not in stop_words]\n",
    "    \n",
    "    # Lemmatize tokens\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Filter out very short tokens\n",
    "    tokens = [token for token in tokens if len(token) > 2]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "print(\"Preprocessing function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the corpus for topic modeling\n",
    "print(\"Preparing corpus for topic modeling...\")\n",
    "\n",
    "documents = []\n",
    "document_categories = []\n",
    "document_ids = []\n",
    "\n",
    "# Process documents from each category\n",
    "for category in categories:\n",
    "    file_ids = brown.fileids(categories=category)\n",
    "    for file_id in file_ids:\n",
    "        # Get raw text\n",
    "        raw_text = ' '.join(brown.words(file_id))\n",
    "        \n",
    "        # Preprocess text\n",
    "        processed_text = preprocess_text(raw_text)\n",
    "        \n",
    "        # Only include documents with sufficient content\n",
    "        if len(processed_text.split()) > 50:\n",
    "            documents.append(processed_text)\n",
    "            document_categories.append(category)\n",
    "            document_ids.append(file_id)\n",
    "\n",
    "print(f\"Total documents prepared: {len(documents)}\")\n",
    "print(f\"Average document length: {np.mean([len(doc.split()) for doc in documents]):.1f} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. NMF (Non-negative Matrix Factorization) Topic Model\n",
    "\n",
    "Let's start with building an NMF model for topic modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF-IDF vectorizer for NMF\n",
    "print(\"Creating TF-IDF matrix for NMF...\")\n",
    "\n",
    "# Parameters for vectorization\n",
    "max_features = 1000  # Limit vocabulary size\n",
    "min_df = 2  # Ignore terms that appear in less than 2 documents\n",
    "max_df = 0.8  # Ignore terms that appear in more than 80% of documents\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=max_features,\n",
    "    min_df=min_df,\n",
    "    max_df=max_df,\n",
    "    ngram_range=(1, 2),  # Include unigrams and bigrams\n",
    "    stop_words='english'\n",
    ")\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(f\"TF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
    "print(f\"Vocabulary size: {len(feature_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check scikit-learn version for NMF parameter compatibility\n",
    "import sklearn\n",
    "print(f\"Scikit-learn version: {sklearn.__version__}\")\n",
    "\n",
    "# Note: NMF parameters changed in different sklearn versions:\n",
    "# - Older versions used 'alpha' parameter\n",
    "# - Newer versions use 'alpha_W' and 'alpha_H' parameters\n",
    "# We'll use basic parameters for maximum compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit NMF model\n",
    "print(\"Fitting NMF model...\")\n",
    "\n",
    "n_topics = 10  # Number of topics to extract\n",
    "random_state = 42\n",
    "\n",
    "# Create NMF model with compatible parameters\n",
    "nmf_model = NMF(\n",
    "    n_components=n_topics,\n",
    "    random_state=random_state,\n",
    "    init='nndsvd',  # Non-negative double SVD initialization\n",
    "    solver='cd',  # Coordinate descent solver\n",
    "    max_iter=200\n",
    ")\n",
    "\n",
    "nmf_topics = nmf_model.fit_transform(tfidf_matrix)\n",
    "\n",
    "print(f\"NMF model fitted successfully!\")\n",
    "print(f\"Document-topic matrix shape: {nmf_topics.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display top words for each NMF topic\n",
    "def display_topics(model, feature_names, n_top_words=10, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Display the top words for each topic.\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== {model_name} Topics ===\")\n",
    "    \n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_words_idx = topic.argsort()[-n_top_words:][::-1]\n",
    "        top_words = [feature_names[i] for i in top_words_idx]\n",
    "        top_weights = [topic[i] for i in top_words_idx]\n",
    "        \n",
    "        print(f\"\\nTopic {topic_idx + 1}:\")\n",
    "        for word, weight in zip(top_words, top_weights):\n",
    "            print(f\"  {word}: {weight:.3f}\")\n",
    "\n",
    "# Display NMF topics\n",
    "display_topics(nmf_model, feature_names, n_top_words=8, model_name=\"NMF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 NMF Topic Interpretation\n",
    "\n",
    "**Analysis of NMF Topics:**\n",
    "\n",
    "*[Add your interpretation of the NMF topics here. Discuss what themes or subjects each topic seems to represent based on the top words.]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LSA (Latent Semantic Analysis) Topic Model\n",
    "\n",
    "Now let's build an LSA model using Truncated SVD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit LSA model using TruncatedSVD\n",
    "print(\"Fitting LSA model...\")\n",
    "\n",
    "lsa_model = TruncatedSVD(\n",
    "    n_components=n_topics,\n",
    "    random_state=random_state,\n",
    "    algorithm='randomized'\n",
    ")\n",
    "\n",
    "lsa_topics = lsa_model.fit_transform(tfidf_matrix)\n",
    "\n",
    "print(f\"LSA model fitted successfully!\")\n",
    "print(f\"Document-topic matrix shape: {lsa_topics.shape}\")\n",
    "print(f\"Explained variance ratio: {lsa_model.explained_variance_ratio_.sum():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display LSA topics\n",
    "display_topics(lsa_model, feature_names, n_top_words=8, model_name=\"LSA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 LSA Topic Interpretation\n",
    "\n",
    "**Analysis of LSA Topics:**\n",
    "\n",
    "*[Add your interpretation of the LSA topics here. Compare with NMF results and discuss similarities/differences.]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LDA (Latent Dirichlet Allocation) Topic Model\n",
    "\n",
    "Finally, let's build an LDA model. LDA works better with count data rather than TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create count vectorizer for LDA\n",
    "print(\"Creating count matrix for LDA...\")\n",
    "\n",
    "count_vectorizer = CountVectorizer(\n",
    "    max_features=max_features,\n",
    "    min_df=min_df,\n",
    "    max_df=max_df,\n",
    "    ngram_range=(1, 1),  # Only unigrams for LDA\n",
    "    stop_words='english'\n",
    ")\n",
    "\n",
    "count_matrix = count_vectorizer.fit_transform(documents)\n",
    "count_feature_names = count_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(f\"Count matrix shape: {count_matrix.shape}\")\n",
    "print(f\"Vocabulary size: {len(count_feature_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit LDA model\n",
    "print(\"Fitting LDA model...\")\n",
    "\n",
    "lda_model = LatentDirichletAllocation(\n",
    "    n_components=n_topics,\n",
    "    random_state=random_state,\n",
    "    alpha=0.1,  # Document-topic concentration\n",
    "    beta=0.01,  # Topic-word concentration\n",
    "    max_iter=100,\n",
    "    learning_method='batch'\n",
    ")\n",
    "\n",
    "lda_topics = lda_model.fit_transform(count_matrix)\n",
    "\n",
    "print(f\"LDA model fitted successfully!\")\n",
    "print(f\"Document-topic matrix shape: {lda_topics.shape}\")\n",
    "print(f\"Log likelihood: {lda_model.score(count_matrix):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display LDA topics\n",
    "display_topics(lda_model, count_feature_names, n_top_words=8, model_name=\"LDA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 LDA Topic Interpretation\n",
    "\n",
    "**Analysis of LDA Topics:**\n",
    "\n",
    "*[Add your interpretation of the LDA topics here. Compare with NMF and LSA results.]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Comparison and Analysis\n",
    "\n",
    "Let's compare the three topic modeling approaches and analyze their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comparison of topic assignments\n",
    "def get_dominant_topic(doc_topic_matrix):\n",
    "    \"\"\"\n",
    "    Get the dominant topic for each document.\n",
    "    \"\"\"\n",
    "    return np.argmax(doc_topic_matrix, axis=1)\n",
    "\n",
    "# Get dominant topics for each model\n",
    "nmf_dominant_topics = get_dominant_topic(nmf_topics)\n",
    "lsa_dominant_topics = get_dominant_topic(lsa_topics)\n",
    "lda_dominant_topics = get_dominant_topic(lda_topics)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame({\n",
    "    'document_id': document_ids,\n",
    "    'true_category': document_categories,\n",
    "    'nmf_topic': nmf_dominant_topics,\n",
    "    'lsa_topic': lsa_dominant_topics,\n",
    "    'lda_topic': lda_dominant_topics\n",
    "})\n",
    "\n",
    "print(\"Topic assignment comparison:\")\n",
    "print(comparison_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze topic distribution by true categories\n",
    "print(\"\\nTopic distribution analysis:\")\n",
    "\n",
    "for model_name, topic_col in [('NMF', 'nmf_topic'), ('LSA', 'lsa_topic'), ('LDA', 'lda_topic')]:\n",
    "    print(f\"\\n{model_name} Topic Distribution by Category:\")\n",
    "    topic_category_crosstab = pd.crosstab(comparison_df['true_category'], comparison_df[topic_col])\n",
    "    print(topic_category_crosstab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize topic distributions\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "models = [('NMF', nmf_topics), ('LSA', lsa_topics), ('LDA', lda_topics)]\n",
    "\n",
    "for idx, (model_name, topic_matrix) in enumerate(models):\n",
    "    # Calculate average topic weights\n",
    "    avg_topic_weights = np.mean(topic_matrix, axis=0)\n",
    "    \n",
    "    axes[idx].bar(range(len(avg_topic_weights)), avg_topic_weights)\n",
    "    axes[idx].set_title(f'{model_name} Average Topic Weights')\n",
    "    axes[idx].set_xlabel('Topic')\n",
    "    axes[idx].set_ylabel('Average Weight')\n",
    "    axes[idx].set_xticks(range(len(avg_topic_weights)))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusions and Insights\n",
    "\n",
    "### 6.1 Model Comparison Summary\n",
    "\n",
    "**NMF (Non-negative Matrix Factorization):**\n",
    "- *[Add your analysis of NMF performance and characteristics]*\n",
    "\n",
    "**LSA (Latent Semantic Analysis):**\n",
    "- *[Add your analysis of LSA performance and characteristics]*\n",
    "\n",
    "**LDA (Latent Dirichlet Allocation):**\n",
    "- *[Add your analysis of LDA performance and characteristics]*\n",
    "\n",
    "### 6.2 Comparison with Official Brown Corpus Categories\n",
    "\n",
    "*[Discuss how well each model's topics align with the official Brown corpus categories. Which model performed best at capturing the underlying document structure?]*\n",
    "\n",
    "### 6.3 Key Findings\n",
    "\n",
    "1. *[Finding 1]*\n",
    "2. *[Finding 2]*\n",
    "3. *[Finding 3]*\n",
    "\n",
    "### 6.4 Recommendations\n",
    "\n",
    "*[Based on your analysis, which topic modeling approach would you recommend for different use cases?]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Additional Analysis (Optional)\n",
    "\n",
    "### 7.1 Topic Coherence Analysis\n",
    "\n",
    "*[If time permits, add topic coherence analysis or other advanced metrics]*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Add any additional analysis code here\n",
    "print(\"Assignment completed successfully!\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Fill in the interpretation sections with your analysis\")\n",
    "print(\"2. Run all cells and verify results\")\n",
    "print(\"3. Convert notebook to PDF for submission\")\n",
    "print(\"4. Commit and push to GitHub\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
